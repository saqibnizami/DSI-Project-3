{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, json, praw, pprint, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psaw import PushshiftAPI    # PSAW recommended by following PRAW errors\n",
    "import datetime as dt            # PSAW docs\n",
    "from IPython.display import display\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing reddit API\n",
    "\n",
    "reddit = praw.Reddit(client_id='CmKUgfSklwH6Gw',\n",
    "                     client_secret='WprZwImA7V8TcggsN0GfpZOfl2g',\n",
    "                     user_agent='ClassProjectBot-PRAW/PSAW',\n",
    "                     password='dsBaLpQSua2ctCXU2XyupJ',\n",
    "                     username='refused_dev')\n",
    "\n",
    "# Set PushShiftAPI\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# Set a variable equal to the target subreddit\n",
    "r_all = reddit.subreddit('all')\n",
    "\n",
    "\n",
    "# Grab submissions from the subreddit\n",
    "subs = []\n",
    "for sub in r_all.hot(limit=None):\n",
    "    sub_dict = {}\n",
    "    sub_dict['submissions'] = sub\n",
    "    subs.append(sub_dict)\n",
    "subscrape = pd.DataFrame(subs)\n",
    "subscrape.shape, subscrape.head()\n",
    "\n",
    "# Save submission pull to csv\n",
    "subscrape.to_csv(\"subs.csv\")\n",
    "\n",
    "# Grab features from the submission IDs\n",
    "sublist = []\n",
    "for c in subscrape['submissions']:\n",
    "    subdict = {}\n",
    "    subdict['title'] = c.title\n",
    "    subdict['comments'] = c.num_comments\n",
    "    subdict['crossposts'] = c.num_crossposts\n",
    "    subdict['score'] = c.score\n",
    "    subdict['subreddit'] = c.subreddit\n",
    "    subdict['domain'] = c.domain\n",
    "    subdict['gilded'] = c.gilded\n",
    "    subdict['upvote_ratio'] = c.upvote_ratio\n",
    "    subdict['created'] = c.created\n",
    "    fri_sublist.append(subdict)\n",
    "\n",
    "df = pd.DataFrame(sublist)\n",
    "\n",
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "stamp = data['created'].apply(get_date)\n",
    "data = data.assign(timestamp = stamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up previous scrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22767, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comments</th>\n",
       "      <th>created</th>\n",
       "      <th>crossposts</th>\n",
       "      <th>domain</th>\n",
       "      <th>gilded</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3232</td>\n",
       "      <td>1.527744e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>5</td>\n",
       "      <td>83372</td>\n",
       "      <td>videos</td>\n",
       "      <td>Gamer bet if the new Bethesda reveal was not F...</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2439</td>\n",
       "      <td>1.527740e+09</td>\n",
       "      <td>6</td>\n",
       "      <td>smh.com.au</td>\n",
       "      <td>0</td>\n",
       "      <td>61556</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>Police faked 258,000 breath tests in shocking ...</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>1.527746e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>0</td>\n",
       "      <td>54696</td>\n",
       "      <td>BikiniBottomTwitter</td>\n",
       "      <td>About to make a splash? More like just about t...</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3771</td>\n",
       "      <td>1.527740e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>0</td>\n",
       "      <td>39344</td>\n",
       "      <td>politics</td>\n",
       "      <td>FBI is reconstructing shredded documents obtai...</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>275</td>\n",
       "      <td>1.527748e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>0</td>\n",
       "      <td>31701</td>\n",
       "      <td>PrequelMemes</td>\n",
       "      <td>Always 2 there are, no more, no less</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  comments       created  crossposts       domain  gilded  score  \\\n",
       "2           2      3232  1.527744e+09           4  youtube.com       5  83372   \n",
       "3           3      2439  1.527740e+09           6   smh.com.au       0  61556   \n",
       "0           0       268  1.527746e+09           1    i.redd.it       0  54696   \n",
       "1           1      3771  1.527740e+09           2  thehill.com       0  39344   \n",
       "4           4       275  1.527748e+09           0    i.redd.it       0  31701   \n",
       "\n",
       "             subreddit                                              title  \\\n",
       "2               videos  Gamer bet if the new Bethesda reveal was not F...   \n",
       "3            worldnews  Police faked 258,000 breath tests in shocking ...   \n",
       "0  BikiniBottomTwitter  About to make a splash? More like just about t...   \n",
       "1             politics  FBI is reconstructing shredded documents obtai...   \n",
       "4         PrequelMemes               Always 2 there are, no more, no less   \n",
       "\n",
       "   upvote_ratio  \n",
       "2          0.78  \n",
       "3          0.89  \n",
       "0          0.82  \n",
       "1          0.89  \n",
       "4          0.85  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('df_w_feats.csv')\n",
    "display(df.shape, df.head().sort_values('score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "df.info()\n",
    "\n",
    "# convert unix timecode to human readable\n",
    "\n",
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "stamp = df['created'].apply(get_date)\n",
    "df = df.assign(timestamp = stamp)\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.head()\n",
    "\n",
    "df['datetime'] = df.timestamp.values.astype('datetime64[D]')\n",
    "\n",
    "df.timestamp.\n",
    "\n",
    "y = df['comments']\n",
    "# y.tolist()\n",
    "\n",
    "# y = pd.DataFrame(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=.20,\n",
    "                                                    random_state=19)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.shape, y_train.shape\n",
    "\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "dft = X_train.copy()\n",
    "\n",
    "jtplot.style()\n",
    "# setting axis range in seaborn:\n",
    "# sns.distplot(dft.comments, kde=True).set(xlim=(0, 1000), ylim=(0, 0.0025));\n",
    "\n",
    "\n",
    "\n",
    "dft_coms = dft.query('comments < 5000 & comments > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtrainer(dft, y):\n",
    "    # calculate percentiles\n",
    "\n",
    "    p25, p50, p75 = np.percentile(dft.comments, 25), np.percentile(dft.comments, 50), np.percentile(dft.comments, 75)\n",
    "\n",
    "    print(p25, p50, p75)\n",
    "\n",
    "\n",
    "\n",
    "    dft['p25'] = dft['comments'] <= p25\n",
    "    dft['p50'] = dft['comments'] <= p50\n",
    "    dft['p75'] = dft['comments'] <= p75\n",
    "\n",
    "    # posts with over the median amount of comments are a success\n",
    "    dft['success'] = dft['p50'].map({False : 1, True : 0})\n",
    "    dft['over25p'] = dft['p25'].map({False : 1, True : 0})\n",
    "    dft['over50p'] = dft['p50'].map({False : 1, True : 0})\n",
    "    dft['over75p'] = dft['p75'].map({False : 1, True : 0})\n",
    "\n",
    "    y = dft['success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18213,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(18213,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19058           FortNiteBR\n",
       "15420    ShitPostCrusaders\n",
       "22607    PropagandaPosters\n",
       "20418         Ice_Poseidon\n",
       "10071      rupaulsdragrace\n",
       "Name: subreddit, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "19058    1\n",
       "15420    1\n",
       "22607    1\n",
       "20418    0\n",
       "10071    1\n",
       "Name: success, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dft.drop(columns='status', inplace=True)\n",
    "\n",
    "X_train = dft.subreddit\n",
    "\n",
    "display(X_train.shape, y_train.shape, X_train.head(), y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a RandomForest model to predict High/Low number of comments using only\n",
    "# the subreddit as a feature\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18213, 3288) 3288\n"
     ]
    }
   ],
   "source": [
    "xtrain_counts = cvec.fit_transform(X_train)\n",
    "print( xtrain_counts.shape, len(cvec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1200isjerky</th>\n",
       "      <th>13or30</th>\n",
       "      <th>13reasonswhy</th>\n",
       "      <th>18_19</th>\n",
       "      <th>195</th>\n",
       "      <th>2007scape</th>\n",
       "      <th>2booty</th>\n",
       "      <th>2busty2hide</th>\n",
       "      <th>2healthbars</th>\n",
       "      <th>2mad4madlads</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtubehaiku</th>\n",
       "      <th>yugioh</th>\n",
       "      <th>yuri</th>\n",
       "      <th>yurop</th>\n",
       "      <th>yuzumiko</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zerowaste</th>\n",
       "      <th>zettairyouiki</th>\n",
       "      <th>zoomies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1200isjerky  13or30  13reasonswhy  18_19  195  2007scape  2booty  \\\n",
       "0            0       0             0      0    0          0       0   \n",
       "1            0       0             0      0    0          0       0   \n",
       "2            0       0             0      0    0          0       0   \n",
       "3            0       0             0      0    0          0       0   \n",
       "4            0       0             0      0    0          0       0   \n",
       "\n",
       "   2busty2hide  2healthbars  2mad4madlads   ...     youtube  youtubehaiku  \\\n",
       "0            0            0             0   ...           0             0   \n",
       "1            0            0             0   ...           0             0   \n",
       "2            0            0             0   ...           0             0   \n",
       "3            0            0             0   ...           0             0   \n",
       "4            0            0             0   ...           0             0   \n",
       "\n",
       "   yugioh  yuri  yurop  yuzumiko  zelda  zerowaste  zettairyouiki  zoomies  \n",
       "0       0     0      0         0      0          0              0        0  \n",
       "1       0     0      0         0      0          0              0        0  \n",
       "2       0     0      0         0      0          0              0        0  \n",
       "3       0     0      0         0      0          0              0        0  \n",
       "4       0     0      0         0      0          0              0        0  \n",
       "\n",
       "[5 rows x 3288 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn the cvec'd X_train to a dataframe\n",
    "X_train = pd.DataFrame(xtrain_counts.todense(), columns=cvec.get_feature_names())\n",
    "X_train.head()\n",
    "\n",
    "# sum along the columns, and sort to give the most common subs\n",
    "X_train.sum(axis=0).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19058    1\n",
       "15420    1\n",
       "22607    1\n",
       "20418    0\n",
       "10071    1\n",
       "Name: success, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()\n",
    "\n",
    "xtrainer(X_test, y_test)\n",
    "\n",
    "y_test = X_test['success']\n",
    "\n",
    "display(X_test.head(), y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_counts = cvec.transform(X_test)\n",
    "X_test = pd.DataFrame(xtest_counts.todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4554, 18]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-d1a8eb75a072>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4554, 18]"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline stuff\n",
    "# use a randomforest\n",
    "cvec = CountVectorizer()\n",
    "tfvect = TfidfVectorizer(stop_words='english')\n",
    "rf = RandomForestClassifier()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "tube = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words = 'english')),\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('lr', LogisticRegression())\n",
    "    sd\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfidf__min_df': ,\n",
    "    'tfidf_max_df': ,\n",
    "    'rf__max_depth': ,\n",
    "    'rf__min_samples_',\n",
    "    'lr__penalty':['l1','l2'],\n",
    "    'lr__'\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(tube, param_grid=params)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
